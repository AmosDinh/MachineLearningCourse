{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden wir bezüglich Pytorch lernen:\n",
    "- Methoden zum Rechnen mit Matrizen\n",
    "- Indexoperationen auf Matrizen\n",
    "- Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rechnen mit Matrizen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tensoren erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), torch.Size([2, 2]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[5, 6], [7, 8]])\n",
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3392, -0.5431],\n",
       "         [-0.4813, -1.6955]]),\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1 = torch.randn(2, 2)  # Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution)\n",
    "C2 = torch.zeros(2, 2)  \n",
    "C1, C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C3 = torch.zeros_like(A)  # Returns a tensor filled with zeros with the same shape as input\n",
    "C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.]]),\n",
       " tensor([[1, 1],\n",
       "         [1, 1]]),\n",
       " torch.float32,\n",
       " torch.int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Einsen\n",
    "C4 = torch.ones(2, 2)  # Returns a tensor filled with ones\n",
    "C5 = torch.ones_like(A)  # Returns a tensor filled with ones with the same shape as input matrix A\n",
    "C4, C5, C4.dtype, C5.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Einheitsmatrix\n",
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [0, 2, 0, 0],\n",
       "        [0, 0, 3, 0],\n",
       "        [0, 0, 0, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diagonalmatrix von Vektor\n",
    "D = torch.tensor([1,2,3,4])\n",
    "torch.diag(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(D).diag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Arbeiten mit Dimensionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 3],\n",
       "         [2, 4]]),\n",
       " tensor([[1, 3],\n",
       "         [2, 4]]),\n",
       " tensor([[1, 3],\n",
       "         [2, 4]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose \n",
    "A.T, torch.t(A), torch.transpose(A, 0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 4, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.zeros(2,4,5,7)\n",
    "A.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4]) torch.Size([4])\n",
      "tensor([[1, 2, 3, 4]]) torch.Size([1, 4])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]]) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "# 0 d vs 1 d\n",
    "print(D, D.shape)\n",
    "# Eine Dimension hinzufügen\n",
    "print(D.unsqueeze(0), D.unsqueeze(0).shape)\n",
    "print(D.unsqueeze(1), D.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Squeeze <-> Unsqueeze rückgängig\n",
    "D1 = D.unsqueeze(0)\n",
    "print(D1.squeeze(0), D1.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "+\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "=\n",
      "tensor([[2., 1., 1., 1.],\n",
      "        [2., 3., 2., 2.],\n",
      "        [3., 3., 4., 3.],\n",
      "        [4., 4., 4., 5.]])\n",
      "In Shapes:\n",
      "torch.Size([4, 1])\n",
      "+\n",
      "torch.Size([4, 4])\n",
      "=\n",
      "torch.Size([4, 4])\n",
      "\n",
      "-> Die 2. Dimension von E wird 'vervielfacht'\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting bei elementweisen Operationen: Addition, Subtraktion, Multiplikation, Division\n",
    "E = D.unsqueeze(1) \n",
    "\n",
    "F = torch.eye(4)\n",
    "print(E)\n",
    "print(\"+\")\n",
    "print(F)\n",
    "print(\"=\")\n",
    "print(E+F)\n",
    "\n",
    "print(\"In Shapes:\")\n",
    "print(E.shape)\n",
    "print('+')\n",
    "print(F.shape)\n",
    "print('=')\n",
    "print((E+F).shape)\n",
    "print(\"\\n-> Die 2. Dimension von E wird 'vervielfacht'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4]])\n",
      "+\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "=\n",
      "tensor([[2., 2., 3., 4.],\n",
      "        [1., 3., 3., 4.],\n",
      "        [1., 2., 4., 4.],\n",
      "        [1., 2., 3., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting bei elementweisen Operationen: Addition, Subtraktion, Multiplikation, Division\n",
    "E = D.unsqueeze(0) \n",
    "\n",
    "F = torch.eye(4)\n",
    "print(E)\n",
    "print(\"+\")\n",
    "print(F)\n",
    "print(\"=\")\n",
    "print(E+F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besser nicht mit 0-Dimensionalen Tensoren arbeiten! (Hier kann man schnell durcheinander kommen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 1., 1., 1.],\n",
       "         [1., 2., 1., 1.],\n",
       "         [1., 1., 2., 1.],\n",
       "         [1., 1., 1., 2.]]),\n",
       " tensor([[ 0., -1., -1., -1.],\n",
       "         [-1.,  0., -1., -1.],\n",
       "         [-1., -1.,  0., -1.],\n",
       "         [-1., -1., -1.,  0.]]),\n",
       " tensor([[2.7500, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 2.7500, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 2.7500, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 2.7500]]),\n",
       " tensor([[0.5000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.5000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mehr Broadcasting Beispiele\n",
    "F + 1, F - 1, F * 2.75, F / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Matrix Multiplikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (40x7 and 2x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Matrix Multiplikation \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m, A \u001b[38;5;241m@\u001b[39m B, torch\u001b[38;5;241m.\u001b[39mmm(A, B),  \u001b[38;5;66;03m# .mm nur für 2D Tensoren\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nutze .matmul (Klarer)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (40x7 and 2x2)"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplikation \n",
    "torch.matmul(A, B), A @ B, torch.mm(A, B),  # .mm nur für 2D Tensoren\n",
    "# nutze .matmul (Klarer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2025-03-02-17-46-53.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 13, 9, 2, 4])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(12,13,9, 2, 3)  \n",
    "B = torch.randn(12,13,9, 3, 4)  \n",
    "C = torch.matmul(A, B)  \n",
    "C.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (9) must match the size of tensor b (10) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \n\u001b[1;32m      2\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)  \n\u001b[0;32m----> 3\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m  \n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (10) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "A = torch.randn(12,13,9, 2, 3)  \n",
    "B = torch.randn(12,13,10, 3, 4)  \n",
    "C = torch.matmul(A, B)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8., 0., 0.],\n",
       "        [0., 8., 0.],\n",
       "        [0., 0., 8.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_power(torch.eye(3)*2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noch viele viele mehr Operationen:\n",
    "# torch.inverse, torch.det, torch.eig, torch.svd,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.35 Elementwise Matrix Multiplikation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5, 12],\n",
       "         [21, 32]]),\n",
       " tensor([[ 5, 12],\n",
       "         [21, 32]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = torch.tensor([[1, 2], [3, 4]])\n",
    "B1 = torch.tensor([[5, 6], [7, 8]])\n",
    "A1 * B1, torch.mul(A1, B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Aggregatoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10.),\n",
       " tensor(2.5000),\n",
       " tensor(1.2910),\n",
       " tensor(1.6667),\n",
       " tensor(4.),\n",
       " tensor(1.),\n",
       " tensor(3),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Einige nützliche Funktionen\n",
    "T = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "T.sum(), T.mean(), T.std(), T.var(), T.max(), T.min(), T.argmax(), T.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: tensor([[1., 2., 3.],\n",
      "        [6., 5., 4.],\n",
      "        [7., 8., 9.]])\n",
      "M.sum(): tensor(45.)\n",
      "M.sum(dim=0): tensor([14., 15., 16.])\n",
      "M.sum(dim=1): tensor([ 6., 15., 24.])\n"
     ]
    }
   ],
   "source": [
    "M = torch.tensor([[1,2,3], [6,5,4], [7,8,9]], dtype=torch.float32)\n",
    "print('M:', M)\n",
    "print('M.sum():', M.sum())\n",
    "print('M.sum(dim=0):', M.sum(dim=0))\n",
    "print('M.sum(dim=1):', M.sum(dim=1))\n",
    "\n",
    "# Analog für mean, std, var, max, min, argmax, argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: tensor([[4., 9., 2.],\n",
      "        [3., 5., 7.],\n",
      "        [8., 1., 6.]])\n",
      "M.sum(): tensor(45.)\n",
      "M.sum(dim=0): tensor([15., 15., 15.])\n",
      "M.sum(dim=1): tensor([15., 15., 15.])\n",
      "M.diag().sum(): tensor(15.)\n"
     ]
    }
   ],
   "source": [
    "# Magisches Quadrat \n",
    "M = torch.tensor([[4,9,2], [3,5,7], [8,1,6]], dtype=torch.float32)\n",
    "print('M:', M)\n",
    "print('M.sum():', M.sum())\n",
    "print('M.sum(dim=0):', M.sum(dim=0))\n",
    "print('M.sum(dim=1):', M.sum(dim=1))\n",
    "print('M.diag().sum():', M.diag().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: tensor([[1., 2., 3.],\n",
      "        [6., 5., 4.],\n",
      "        [7., 9., 8.]])\n",
      "M.argmax(): tensor(7)\n",
      "M.argmax(dim=0): tensor([2, 2, 2])\n",
      "M.argmax(dim=1): tensor([2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Was ist argmax? Gibt den Index des größten Elements im Tensor zurück\n",
    "M = torch.tensor([[1,2,3], [6,5,4], [7,9,8]], dtype=torch.float32)\n",
    "print('M:', M)\n",
    "print('M.argmax():', M.argmax())\n",
    "print('M.argmax(dim=0):', M.argmax(dim=0))\n",
    "print('M.argmax(dim=1):', M.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Indexierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [6., 5., 4.],\n",
      "        [7., 9., 8.]])\n",
      "M[0,0]: tensor(1.)\n",
      "M[1,0]: tensor(6.)\n",
      "M[2,2]: tensor(8.)\n",
      "M[1, :]: tensor([6., 5., 4.])\n",
      "M[:, 1]: tensor([2., 5., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Indexierung startet bei 0\n",
    "# M[0,0] -> 1. Zeile, 1. Spalte\n",
    "# M[1,0] -> 2. Zeile, 1. Spalte\n",
    "# M[2,2] -> 3. Zeile, 3. Spalte\n",
    "# : wird verwendet um alle Indices einer Dimension zu selektieren \n",
    "# M[1, :] -> 2. Zeile, alle Spalten\n",
    "# M[:, 1] -> alle Zeilen, 2. Spalte\n",
    "\n",
    "M = torch.tensor([[1,2,3], [6,5,4], [7,9,8]], dtype=torch.float32)\n",
    "print( M)\n",
    "print('M[0,0]:', M[0,0])\n",
    "print('M[1,0]:', M[1,0])\n",
    "print('M[2,2]:', M[2,2])\n",
    "print('M[1, :]:', M[1, :])\n",
    "print('M[:, 1]:', M[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "A[2:5]: tensor([3, 4, 5])\n",
      "A[:5]: tensor([1, 2, 3, 4, 5])\n",
      "A[5:]: tensor([ 6,  7,  8,  9, 10])\n",
      "A[-1]: tensor(10)\n",
      "A[-3:]: tensor([ 8,  9, 10])\n",
      "A[:-3]: tensor([1, 2, 3, 4, 5, 6, 7])\n",
      "\n",
      "tensor([[1., 2., 3.],\n",
      "        [6., 5., 4.],\n",
      "        [7., 9., 8.]])\n",
      "M[:, 0:2]: tensor([[1., 2.],\n",
      "        [6., 5.],\n",
      "        [7., 9.]])\n"
     ]
    }
   ],
   "source": [
    "# Slicing, nutze  \"a:b\" um die Elemente von Index a bis b-1 zu selektieren\n",
    "A = torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "print(A)\n",
    "print('A[2:5]:', A[2:5])\n",
    "print('A[:5]:', A[:5])\n",
    "print('A[5:]:', A[5:])\n",
    "\n",
    "# Nutze - um von hinten zu zählen\n",
    "print('A[-1]:', A[-1])\n",
    "print('A[-3:]:', A[-3:])\n",
    "print('A[:-3]:', A[:-3], end='\\n\\n')\n",
    "\n",
    "# Analog für mehrdimensionale Tensoren ...\n",
    "print(M)\n",
    "print('M[:, 0:2]:', M[:, 0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([100,  90,  80,  70,  60,  50,  40,  30,  20,  10]),\n",
       " tensor([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zur Vereinfachung wird folgendes Beispiel nur auf 1D Tensoren angewendet, ist aber auf mehrdimensionale Tensoren übertragbar\n",
    "A = torch.tensor([1,2,3,4,5,6,7,8,9,10]) *10\n",
    "sortiertes_A, sortierte_Indices = torch.sort(A, descending=True, )\n",
    "\n",
    "sortiertes_A, sortierte_Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([110,  99,  88,  77,  66,  55,  44,  33,  11,  22])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([2,1,3,4,5,6,7,8,9,10]) *11\n",
    "sortierte_Indices = torch.argsort(A, descending=True)\n",
    "nach_A_sortierte_B = B[sortierte_Indices]\n",
    "nach_A_sortierte_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False,  True,  True, False, False, False])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maskierung, \"Masking\"\n",
    "A = torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "mask = (A > 5) & (A < 8)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False, False,  True, False, False,  True, False, False]),\n",
       " tensor([ True,  True,  True,  True, False,  True,  True, False,  True,  True]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (A == 5) | (A == 8) \n",
    "mask, ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False,  True, False, False, False, False, False, False, False]),\n",
       " tensor([3]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mask = torch.zeros(10, dtype=torch.bool)\n",
    "my_mask[2] = True\n",
    "my_mask, A[my_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutze sort(,...., dim=2) # Sortieren entlang der 2. Dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [ True, False, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1,2,3], [6,5,4], [7,9,8]], dtype=torch.float32)\n",
    "A > 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 7., 9., 8.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[A > 5]  # -> Es wird immer 1D tensor zurückgegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [0., 5., 4.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[A > 5] = 0\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [2, 0],\n",
       "        [2, 1],\n",
       "        [2, 2]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argwhere(A > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [0., 5., 4.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [1., 2., 3.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[A > 3] = 0\n",
    "A.unique(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 View, Reshape, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 6., 5., 4., 7., 9., 8.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neben squeeze, unsqueeze gibt es auch flatten, view, reshape\n",
    "# M.argmax() \n",
    "M.flatten() # -> argmax dann definiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2025-03-02-18-01-43.png)\n",
    "https://stackoverflow.com/questions/42479902/what-does-view-do-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktioniert nur, wenn die dimensionen aufgehen:\n",
    "# nxm -> n*m Elemente\n",
    "# Wollen wir z.B. 3x4 -> 2x6, 6x2, 4x3, 12x1, 1x12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 82])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nutze -1, wenn du nicht weißt, wie viele Elemente in der Dimension sein sollen\n",
    "# Wollen in erster Dimension 2 Elemente haben\n",
    "T = torch.randn(246*3)\n",
    "T.view(9,-1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was macht .reshape? Genau das gleiche wie view, aber es kopiert die Daten immer (view schlägt fehl, wenn nicht contiguous), kommt gleich\n",
    "# Call by reference vs Call by value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contigous (Zusammenhängend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix: <br>\n",
    "![](2025-03-05-20-16-58.png) <br>\n",
    "Contigous bedeutet, dass in der Matrix benachbarte Elemente im Speicher benachbart vorliegen:\n",
    "![](2025-03-05-20-17-13.png)\n",
    "\n",
    "<br><br>\n",
    "https://stackoverflow.com/a/26999092/9549296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = A.T\n",
    "\n",
    "A.is_contiguous(), B.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "B.view(1,4)\n",
    "\n",
    "# --> .view() schlägt fehl, da B nicht contiguous ist\n",
    "# --> .reshape() kopiert die Daten in neuen Speicherbereich falls nötig (Checkt Contigous), und funktionert somit auf Tensoren, welche nicht contiguous sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 2, 4]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = B.contiguous()\n",
    "B.view(1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bsp an Operationen, die non-contigouity erzeugen\n",
    "- torch.cat \n",
    "- torch.stack\n",
    "- torch.reshape \n",
    "- torch.permute\n",
    "- torch.transpose\n",
    "\n",
    "Pytorch nimmt contigouity an, weil es dadurch performanter ist. Es muss .contigous() aufgerufen werden, zum checken: .is_contigous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call by reference/ call by value\n",
    "Wird später wichtig, wenn numerische Gradienten berechnet werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  3],\n",
       "        [ 2,  4]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = A.T\n",
    "A[0][0] = 10\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [2, 4]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = A.T.clone()\n",
    "A[0][0] = 10\n",
    "B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10,  2,  3,  4]]), tensor([[10,  2,  3,  4]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = A.view(1,4)\n",
    "C = A.reshape(1,4)\n",
    "A[0][0] = 10\n",
    "B, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 2, 4]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "C = A.T.reshape(1,4)  # .T macht nicht-contiguous\n",
    "A[0][0] = 10\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "Zwei Matrizen in spezifizierter Dimension verbinden. <br>\n",
    "Nutze: <br>\n",
    "torch.cat([A,B,C,D,...],dim=x)\n",
    "\n",
    "(Erzeugt immer neue Kopie der Input-Tensoren)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  4],\n",
       "         [ 2,  5],\n",
       "         [ 3,  6],\n",
       "         [ 7, 10],\n",
       "         [ 8, 11],\n",
       "         [ 9, 12],\n",
       "         [ 7, 10],\n",
       "         [ 8, 11],\n",
       "         [ 9, 12]]),\n",
       " tensor([[ 1,  4,  7, 10],\n",
       "         [ 2,  5,  8, 11],\n",
       "         [ 3,  6,  9, 12]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrizen \"konkatenieren\" torch.cat / torch.concatenate\n",
    "A = torch.tensor([[1,2,3], [4,5,6]])\n",
    "A = A.T\n",
    "B = torch.tensor([[7,8,9], [10,11,12]]).T\n",
    "C = torch.cat((A, B, B ), dim=0)\n",
    "D = torch.cat((A, B), dim=1)\n",
    "C, D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]),\n",
       " torch.Size([3, 2]),\n",
       " torch.Size([9, 2]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, B.shape, C.shape, D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  4],\n",
       "         [ 2,  5],\n",
       "         [ 3,  6],\n",
       "         [ 7, 10],\n",
       "         [ 8, 11],\n",
       "         [ 9, 12]]),\n",
       " tensor([[ 1,  4,  7, 10],\n",
       "         [ 2,  5,  8, 11],\n",
       "         [ 3,  6,  9, 12]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0][0] = 10\n",
    "C, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nützlich in Machine Learning um \"Trainingsdaten\" zu erstellen / Batching\n",
    "torch.stack((A, A, A)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
